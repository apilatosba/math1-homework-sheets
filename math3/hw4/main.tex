\documentclass{article}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz}
\usepackage{array}
\usepackage{mathtools}
\usepackage{graphicx}

\begin{document}
\section*{\huge Homework Sheet 4}
\begin{flushright}
   \textbf{Author: Abdullah Oğuz Topçuoğlu}
\end{flushright}

% Exercise 13 (3 Points)
% Let f : R
% 2 → R be a function defined by
% f(x1, x2) := |x1 − x2| ,
% and consider the two points a := (0, 1) and b := (1, 2) in R
% 2
% .
% (i) Verify that f, a and b satisfy the assumptions of the mean value theorem (1.9.1) for a suitable
% choice of G.
% (ii) Determine θ ∈ (0, 1) such that, for ξ := a + θ(b − a), we have
% f(b) − f(a) = ⟨∇f(ξ), b − a⟩.
% (iii) Is it possible to find a θ as in part (ii) for the two points ea := (−1, −1) and eb := (1, 1)? Justify
% your answer.
% Exercise 14 (4 Points)
% Let f : R
% 2 → R be a function defined by
% f(x1, x2) := e
% 1+x1 + x1 sin(πx2).
% Determine the Taylor polynomial of order 1, and order 2 of f at x
% 0 = (x
% 0
% 1
% , x0
% 2
% ) := (0,
% 1
% 2
% ).
% Exercise 15 (5 Points)
% Let fi
% : R
% 2 → R, i = 1, 2, 3, be functions defined by
% f1(x1, x2) := −x
% 4
% 1 + x
% 2
% 2
% , f2(x1, x2) := −x
% 2
% 2 + cos(x1) and f3(x1, x2) := 1 − x
% 2
% 1
% .
% (i) Check whether (0, 0) is a local extremum point of f1.
% (ii) Show that (0, 0) is an isolated maximum point of f2.
% (iii) Show that (0, 0) is a maximum point of f3, but not an isolated maximum point.
% Exercise 16 (4 Points)
% Let f : R
% 2 → R be a function defined by
% f(x1, x2) := x
% 2
% 1 + 4x2
% e
% x
% 2
% 1+x
% 2
% 2
% .
% Assume that it has already been shown that f is a C
% 2
% function. Specify and classify all the stationary
% points of f, i.e. check for each such point whether it is a local (isolated) minimum point, a local
% (isolated) maximum point or a saddle point. Also check for each local extremum point of f whether
% it is a (“global”) extremum point.

\section*{Exercise 13}
We are given the function:
\begin{align*}
   f : \mathbb{R}^2 \to \mathbb{R}, \quad f(x_1, x_2) := |x_1 - x_2|
\end{align*}

and the points:
\begin{align*}
   a := (0, 1), \quad b := (1, 2)
\end{align*}

\subsection*{(i)}
Choose the \(G\):
\begin{align*}
   G := \{(x_1, x_2) \in \mathbb{R}^2 \mid x_1 - x_2 < 0\}
\end{align*}

It is obvious that \(a, b \in G\).\\
Now we need to show that the line segment connecting \(a\) and \(b\) lies in \(G\).\\
\begin{align*}
   \text{line segment} &= \{a + t(b - a) \mid t \in [0, 1]\} \\
   &= \{(0, 1) + t(1 - 0, 2 - 1) \mid t \in [0, 1]\} \\
   &= \{(t, 1 + t) \mid t \in [0, 1]\}
\end{align*}
which obviously is in \(G\).

\subsection*{(ii)}
When we consider the function \(f\) in the domain \(G\), \(f\) is equal to
\begin{align*}
   f(x_1, x_2) = x_2 - x_1
\end{align*}
Lets calculate the gradient of \(f\)
\begin{align*}
   \nabla f(x_1, x_2) = \begin{pmatrix}
      -1 \\
      1
   \end{pmatrix}
\end{align*}
Now we need to find \(\theta \in (0, 1)\) such that
\begin{align*}
   f(b) - f(a) = \langle \nabla f(\xi), b - a \rangle
\end{align*}
where \(\xi := a + \theta(b - a)\).\\
Calculating the left hand side:
\begin{align*}
   f(b) - f(a) &= f(1, 2) - f(0, 1) \\
   &= 1 - 1 = 0
\end{align*}
Calculating the right hand side:
\begin{align*}
   \langle \nabla f(\xi), b - a \rangle &= \langle \nabla f(a + \theta(b - a)), b - a \rangle \\
   &= \langle \nabla f((0, 1) + \theta(1, 1)), (1, 1) \rangle \\
      &= \langle \nabla f(\theta, 1 + \theta), (1, 1) \rangle \\
   &= \langle \begin{pmatrix}
      -1 \\
      1
   \end{pmatrix}, (1, 1) \rangle \\
   &= 0
\end{align*}
We get \(0 = 0\) which is true always and doesnt depend on what \(\theta\) is. So any \(\theta \in (0, 1)\) satisfies the equation.

\subsection*{(iii)}
We are given the points:
\begin{align*}
   \tilde{a} := (-1, -1), \quad \tilde{b} := (1, 1)
\end{align*}

No, we cant directly use the part (ii) here because simply the points \(\tilde{a}\) and \(\tilde{b}\) are not in \(G\) we chose. And actually there is no
\(G\) that contains a point in the diagonal line (where \(x_1 = x_2\)) because \(f\) is not differentiable on that line.

\section*{Exercise 14}
We are given the function
\begin{align*}
   f : \mathbb{R}^2 \to \mathbb{R}, \quad f(x_1, x_2) := e^{1 + x_1} + x_1 \sin(\pi x_2)
\end{align*}

Taylor polynomial of order 1 and 2 is given by
\begin{align*}
   T_1^f(x^0; y) &= f(x^0) + \nabla f(x^0) \cdot (y - x^0) \\
   T_2^f(x^0; y) &= T_1(x^0;y) + \frac{1}{2}(y - x^0)^T H_f(x^0)(y - x^0)
\end{align*}

First and second order ferivaties of \(f\) exist and given by
\begin{align*}
   \frac{\partial f}{\partial x_1} &= e^{1 + x_1} + \sin(\pi x_2) \\
   \frac{\partial f}{\partial x_2} &= \pi x_1 \cos(\pi x_2) \\
   \frac{\partial^2 f}{\partial x_1^2} &= e^{1 + x_1} \\
   \frac{\partial^2 f}{\partial x_2^2} &= -\pi^2 x_1 \sin(\pi x_2) \\
   \frac{\partial^2 f}{\partial x_2 \partial x_1} &= \frac{\partial^2 f}{\partial x_1 \partial x_2} = \pi \cos(\pi x_2) \\
\end{align*}

Since all the partial derivaites are addition and multiplication of other continuous functions, they are continous as well.
\\
\\
\(T_1^f(x^0; y)\) at \((0, \frac{1}{2})\):
\begin{align*}
   f(0, \tfrac{1}{2}) &= e^{1 + 0} + 0 \cdot \sin(\pi \cdot \tfrac{1}{2}) = e \\
   \nabla f(0, \tfrac{1}{2}) &= \begin{pmatrix}
      e^{1 + 0} + \sin(\pi \cdot \tfrac{1}{2}) \\
      \pi \cdot 0 \cdot \cos(\pi \cdot \tfrac{1}{2})
   \end{pmatrix} = \begin{pmatrix}
      e + 1 \\
      0
   \end{pmatrix} \\
   T_1^f((0, \tfrac{1}{2}); (y_1, y_2)) &= e + \begin{pmatrix}
      e + 1 \\
      0
   \end{pmatrix} \cdot \begin{pmatrix}
      y_1 - 0 \\
      y_2 - \tfrac{1}{2}
   \end{pmatrix} = e + (e + 1)y_1
\end{align*}
\\
\(T_2^f(x^0; y)\) at \((0, \frac{1}{2})\):
\begin{align*}
   H_f(0, \tfrac{1}{2}) &= \begin{pmatrix}
      e^{1 + 0} & \pi \cos(\pi \cdot \tfrac{1}{2}) \\
      \pi \cos(\pi \cdot \tfrac{1}{2}) & -\pi^2 \cdot 0 \cdot \sin(\pi \cdot \tfrac{1}{2})
   \end{pmatrix} = \begin{pmatrix}
      e & 0 \\
      0 & 0
   \end{pmatrix} \\
   T_2^f((0, \tfrac{1}{2}); (y_1, y_2)) &= T_1^f((0, \tfrac{1}{2}); (y_1, y_2)) + \frac{1}{2} \begin{pmatrix}
      y_1 - 0 & y_2 - \tfrac{1}{2}
   \end{pmatrix} \begin{pmatrix}
      e & 0 \\
      0 & 0
   \end{pmatrix} \begin{pmatrix}
      y_1 - 0 \\
      y_2 - \tfrac{1}{2}
   \end{pmatrix} \\
   &= e + (e + 1)y_1 + \frac{1}{2} e (y_1)^2
\end{align*}

\section*{Exercise 15}
We are given the functions
\begin{align*}
   f_1(x_1, x_2) &:= -x_1^4 + x_2^2 \\
   f_2(x_1, x_2) &:= -x_2^2 + \cos(x_1) \\
   f_3(x_1, x_2) &:= 1 - x_1^2
\end{align*}

\subsection*{(i)}
To check whether or not a point is local extremum point we are first gonna check if it is a stationary point. If it is then we are gonna check whether or not
the hessian matrix is positive definite or negative definite. \\
First things first \(f_1\) is at least two times continuously differentiable since all the partial derivatives exist and are continuous because they are addition and multiplication of other continuous functions (polynomial functions in this case).\\
\textbf{The gradient of \(f_1\)}
\begin{align*}
   \nabla f_1(x_1, x_2) = \begin{pmatrix}
      \frac{\partial f_1}{\partial x_1} \\
      \\
      \frac{\partial f_1}{\partial x_2}
   \end{pmatrix} = \begin{pmatrix}
      -4x_1^3 \\
      2x_2
   \end{pmatrix}
\end{align*}

Setting the gradient to zero to find stationary points
\begin{align*}
   \nabla f_1(x_1, x_2) = 0 &\implies \begin{pmatrix}
      -4x_1^3 \\
      2x_2
   \end{pmatrix} = \begin{pmatrix}
      0 \\
      0
   \end{pmatrix} \\
   &\implies (x_1, x_2) = (0, 0)
\end{align*}
There is only one stationary point which is \((0, 0)\).\\
Now we need to check if it is a local extremum. \\
\textbf{The hessian matrix of \(f_1\)}
\begin{align*}
   H_{f_1}(x_1, x_2) = \begin{pmatrix}
      \frac{\partial^2 f_1}{\partial x_1^2} & \frac{\partial^2 f_1}{\partial x_1 \partial x_2} \\
      \\
      \frac{\partial^2 f_1}{\partial x_2 \partial x_1} & \frac{\partial^2 f_1}{\partial x_2^2}
   \end{pmatrix} = \begin{pmatrix}
      -12x_1^2 & 0 \\
      0 & 2
   \end{pmatrix}
\end{align*}
at point zero
\begin{align*}
   H_{f_1}(0, 0) = \begin{pmatrix}
      0 & 0 \\
      0 & 2
   \end{pmatrix}
\end{align*}
The hessian matrix is not positive definite nor negative definite since the leading principal minor of order 1 is zero. So it is neither local minimum or local maximum point.\\

\subsection*{(ii)}
\(f_2\) is at least two times continuously differentiable since all the partial derivatives exist and are continuous because they are addition and multiplication of other continuous functions (polynomial and trigonometric functions in this case).\\
\textbf{The gradient of \(f_2\)}
\begin{align*}
   \nabla f_2(x_1, x_2) = \begin{pmatrix}
      \frac{\partial f_2}{\partial x_1} \\
      \\
      \frac{\partial f_2}{\partial x_2}
   \end{pmatrix} = \begin{pmatrix}
      -\sin(x_1) \\
      -2x_2
   \end{pmatrix}
\end{align*}
Setting the gradient to zero to find stationary points
\begin{align*}
   \nabla f_2(x_1, x_2) = 0 &\implies \begin{pmatrix}
      -\sin(x_1) \\
      -2x_2
   \end{pmatrix} = \begin{pmatrix}
      0 \\
      0
   \end{pmatrix} \\
   &\implies (x_1, x_2) = (k\pi, 0), \quad k \in \mathbb{Z}
\end{align*}
There are infinitely many stationary points which are \((k\pi, 0)\) where \(k \in \mathbb{Z}\).\\
\((0, 0)\) is one of the stationary points. To show that it is an isolated maximum point we need to show that the hessian matrix at that point is negative definite.\\
\textbf{The hessian matrix of \(f_2\)}
\begin{align*}
   H_{f_2}(x_1, x_2) = \begin{pmatrix}
      \frac{\partial^2 f_2}{\partial x_1^2} & \frac{\partial^2 f_2}{\partial x_1 \partial x_2} \\
      \\
      \frac{\partial^2 f_2}{\partial x_2 \partial x_1} & \frac{\partial^2 f_2}{\partial x_2^2}
   \end{pmatrix} = \begin{pmatrix}
      -\cos(x_1) & 0 \\
      0 & -2
   \end{pmatrix}
\end{align*}
at point zero
\begin{align*}
   H_{f_2}(0, 0) = \begin{pmatrix}
      -1 & 0 \\
      0 & -2
   \end{pmatrix}
\end{align*}
The leading principal minors are \(-1 < 0\) and \(2 > 0\). They alternate in sign starting with a negative. So the hessian matrix is negative definite. Thus \((0, 0)\) is an isolated maximum point.\\

\subsection*{(iii)}
We know that for any \(r \in \mathbb{R} \quad r^2 \geq 0\) which implies \(1 - r^2 \leq 1\) for any real number \(r\).\\
Meaning that \(f_3(x_1, x_2) = 1 - x_1^2 \leq 1\) and when \(x_1 = 0\) we have \(f_3(0, x_2) = 1\). So if we choose any point in the form of \((0, x_2)\) where \(x_2 \in \mathbb{R}\) we get the maximum value of the function which is \(1\).\\
This makes \((0, 0)\) a maximum point but not an isolated maximum point since there are infinitely many points in the form of \((0, x_2)\) where \(x_2 \in \mathbb{R}\) that gives the same maximum value of \(1\).

\section*{Exercise 16}
We are given the function
\begin{align*}
   f : \mathbb{R}^2 \to \mathbb{R}, \quad f(x_1, x_2) := \frac{x_1^2 + 4x_2}{e^{x_1^2 + x_2^2}}
\end{align*}
Assuming it is already shown that \(f\) is a \(C^2\) function. \\
Lets find where the gradient is zero.\\
\textbf{The gradient of \(f\)}
\begin{align*}
   \nabla f(x_1, x_2) = \begin{pmatrix}
      \frac{\partial f}{\partial x_1} \\
      \\
      \frac{\partial f}{\partial x_2}
   \end{pmatrix} = \begin{pmatrix}
      \frac{2x_1 e^{x_1^2 + x_2^2} - (x_1^2 + 4x_2) \cdot 2x_1 e^{x_1^2 + x_2^2}}{(e^{x_1^2 + x_2^2})^2} \\
      \\
      \frac{4 e^{x_1^2 + x_2^2} - (x_1^2 + 4x_2) \cdot 2x_2 e^{x_1^2 + x_2^2}}{(e^{x_1^2 + x_2^2})^2}
   \end{pmatrix}
\end{align*}
Setting the gradient to zero to find stationary points
\begin{align*}
   \nabla f(x_1, x_2) = 0 &\implies \begin{pmatrix}
      \frac{2x_1 e^{x_1^2 + x_2^2} - (x_1^2 + 4x_2) \cdot 2x_1 e^{x_1^2 + x_2^2}}{(e^{x_1^2 + x_2^2})^2} \\
      \\
      \frac{4 e^{x_1^2 + x_2^2} - (x_1^2 + 4x_2) \cdot 2x_2 e^{x_1^2 + x_2^2}}{(e^{x_1^2 + x_2^2})^2}
   \end{pmatrix} = \begin{pmatrix}
      0 \\
      0
   \end{pmatrix} \shortintertext{since \((e^{x_1^2 + x_2^2})^2\) always positive} \\
   &\implies \begin{pmatrix}
      2x_1 e^{x_1^2 + x_2^2} - (x_1^2 + 4x_2) \cdot 2x_1 e^{x_1^2 + x_2^2} \\
      \\
      4 e^{x_1^2 + x_2^2} - (x_1^2 + 4x_2) \cdot 2x_2 e^{x_1^2 + x_2^2}
   \end{pmatrix} = \begin{pmatrix}
      0 \\
      0
   \end{pmatrix} \\
   &\implies \begin{pmatrix}
      2x_1 - (x_1^2 + 4x_2) \cdot 2x_1 \\
      \\
      4 - (x_1^2 + 4x_2) \cdot 2x_2
   \end{pmatrix} = \begin{pmatrix}
      0 \\
      0
   \end{pmatrix} \\
   &\implies \begin{pmatrix}
      2x_1(1 - x_1^2 - 4x_2) \\
      \\
      4 - 2x_2(x_1^2 + 4x_2)
   \end{pmatrix} = \begin{pmatrix}
      0 \\
      0
   \end{pmatrix}
\end{align*}

From the first equation we have two cases:
\begin{itemize}
   \item Case 1: \(x_1 = 0\)
   \item Case 2: \(1 - x_1^2 - 4x_2 = 0 \implies x_1^2 + 4x_2 = 1\)
\end{itemize}

\textbf{Case 1: \(x_1 = 0\)}
\begin{align*}
   4 - 2x_2(0^2 + 4x_2) &= 0 \\
   4 - 8x_2^2 &= 0 \\
   x_2^2 &= \frac{1}{2} \\
   x_2 &= \pm \frac{1}{\sqrt{2}}
\end{align*}
So we have two stationary points from case 1:
\begin{align*}
   (0, \tfrac{1}{\sqrt{2}}), \quad (0, -\tfrac{1}{\sqrt{2}})
\end{align*}
\textbf{Case 2: \(x_1^2 + 4x_2 = 1\)}
\begin{align*}
   4 - 2x_2(1) &= 0 \\
   4 - 2x_2 &= 0 \\
   x_2 &= 2
\end{align*}
Substituting \(x_2\) back to the equation of case 2:
\begin{align*}
   x_1^2 + 4(2) &= 1 \\
   x_1^2 + 8 &= 1 \\
   x_1^2 &= -7
\end{align*}
which has no real solution.\\
So the only stationary points are:
\begin{align*}
   (0, \tfrac{1}{\sqrt{2}}), \quad (0, -\tfrac{1}{\sqrt{2}})
\end{align*}
\textbf{Classification of the stationary points} \\
THe hessian matrix of \(f\)
\begin{align*}
   H_f(x_1, x_2) = \begin{pmatrix}
      \frac{\partial^2 f}{\partial x_1^2} & \frac{\partial^2 f}{\partial x_1 \partial x_2} \\
      \\
      \frac{\partial^2 f}{\partial x_2 \partial x_1} & \frac{\partial^2 f}{\partial x_2^2}
   \end{pmatrix}
\end{align*}
Okay this is too computationally heavy :). Lets rewrite \(f\) as
\begin{align*}
   f = uw \shortintertext{where} \\
   u(x_1, x_2) = x_1^2 + 4x_2 \\
   w(x_1, x_2) = e^{-(x_1^2 + x_2^2)}
\end{align*}

Partial derivates:
\begin{align*}
   \frac{\partial f}{\partial x_1} &= 2x_1 w + u \cdot (-2x_1 w) = 2x_1 w (1 - u) \\
   \frac{\partial f}{\partial x_2} &= 4 w + u \cdot (-2x_2 w) = 2w(2 - x_2 u) \\
   \frac{\partial^2 f}{\partial x_1^2} &= 2w(1 - u) + 2x_1 \cdot (-2x_1 w)(1 - u) + 2x_1 w \cdot (-2x_1)\\
   &= 2w(1 - u) - 4x_1^2 w(1 - u) - 4x_1^2 w \\
   &= 2w(1 - u - 2x_1^2(1 - u) + 2x_1^2) \\
   &= 2w(1 - u - 2x_1^2 + 2x_1^2 u + 2x_1^2) \\
   &= 2w(1 - u + 2x_1^2 u) \\
   \frac{\partial^2 f}{\partial x_2^2} &= -2x_2 \cdot 2w(2 - x_2 u) + 2w(-u) \\
   &= -4x_2 w(2 - x_2 u) - 2u w \\
   &= -4x_2 w(2 - x_2 u) - 2u w \\
   &= -4x_2 w + 4x_2^2 u w - 2u w \\
   &= w(-4x_2 + 4x_2^2 u - 2u) \\
   \frac{\partial^2 f}{\partial x_1 \partial x_2} &= -2x_1 \cdot 2w(2 - x_2 u) + 2x_1 w(-u) \\
   &= -4x_1 w(2 - x_2 u) - 2x_1 u w \\
   &= -8x_1 w + 4x_1 x_2 u w - 2x_1 u w \\
   &= -8x_1 w + 2x_1 u w (2x_2 - 1)
\end{align*}

At the stationary points:
\begin{align*}
   H_f(0, y) &= \begin{pmatrix}
      -8e^{-y^2}y & 0 \\
      \\
      0 & e^{-y^2}(16y^3 - 12y)
   \end{pmatrix} \\
   H_f(0, \tfrac{1}{\sqrt{2}}) &= \begin{pmatrix}
      -8e^{-\frac{1}{2}} \cdot \frac{1}{\sqrt{2}} & 0 \\
      \\
      0 & e^{-\frac{1}{2}}(16 \cdot \frac{1}{2\sqrt{2}} - 12 \cdot \frac{1}{\sqrt{2}})
   \end{pmatrix} = \begin{pmatrix}
      -8e^{-\frac{1}{2}} \cdot \frac{1}{\sqrt{2}} & 0 \\
      \\
      0 & -4e^{-\frac{1}{2}} \cdot \frac{1}{\sqrt{2}}
   \end{pmatrix} \\
   H_f(0, -\tfrac{1}{\sqrt{2}}) &= \begin{pmatrix}
      8e^{-\frac{1}{2}} \cdot \frac{1}{\sqrt{2}} & 0 \\
      \\
      0 & e^{-\frac{1}{2}}(-16 \cdot \frac{1}{2\sqrt{2}} + 12 \cdot \frac{1}{\sqrt{2}})
   \end{pmatrix} = \begin{pmatrix}
      8e^{-\frac{1}{2}} \cdot \frac{1}{\sqrt{2}} & 0 \\
      \\
      0 & 4e^{-\frac{1}{2}} \cdot \frac{1}{\sqrt{2}}
   \end{pmatrix}
\end{align*}

Lets take a look at \(H_f(0, \tfrac{1}{\sqrt{2}})\). If we consider the leading principal minors then the the first one is negative and the second one is positive which means that
the hessian matrix is negative definite. So the point \((0, \tfrac{1}{\sqrt{2}})\) is a local isolated maximum point.\\
\\
Now looking at \(H_f(0, -\tfrac{1}{\sqrt{2}})\). If we consider the leading principal minors then the the first one is positive and the second one is also positive which means that
the hessian matrix is positive definite. So the point \((0, -\tfrac{1}{\sqrt{2}})\) is a local isolated minimum point.\\
\\
\textbf{Are they global extremum points?} \\
The thing is if \(f\) is bounded then the local maximum point is also a global maximum point and the local minimum point is also a global minimum point because there is only one
local minimum and one local maximum point.\\
\(f\) is bounded becuse as \(\|(x_1, x_2)\| \to \infty\) the denominator \(e^{x_1^2 + x_2^2}\) grows much faster than the numerator \(x_1^2 + 4x_2\) which makes \(f(x_1, x_2) \to 0\). \\
Thus both local extremum points are also global extremum points.\\

\end{document}