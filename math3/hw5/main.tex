\documentclass{article}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz}
\usepackage{array}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}
\section*{\huge Homework Sheet 5}
\begin{flushright}
   \textbf{Author: Abdullah Oğuz Topçuoğlu}
\end{flushright}

% Exercise 17 (4 Points)
% Let f : R → R be a function defined by
% f(x) := −ex + 2x + 1.
% Solve the equation f(x) = 0 approximately by using two iterations of the Newton method with the
% initial value
% (i) x0 := −1.
% (ii) x0 := 1.
% Exercise 18 (4 Points)
% Let f : R2 → R2 be a function defined by
% f(x1, x2) :=
% 󰀗 x3
% 1 + x2 − 2
% x1 + x3
% 2 − 2
% 󰀘
% .
% Solve the equation f(x1, x2) = (0, 0) approximately by using two iterations of the Newton method
% with the initial value x0 := (1, 1).
% Exercise 19 (4 Points)
% Let the functions f : R2 → R and g : R2 → R be defined by
% f(x1, x2) := 1
% 3
% x3
% 1 +
% 1
% 4
% x2
% 2 + x1 +
% 1
% 2
% x2 + 2 ,
% g(x1, x2) := x1 +
% 1
% 2
% x2.
% Use the substitution method to determine the local extremum points of f under the constraint
% g(x1, x2) = 0, i.e. to find all local extremum points of the restriction f|M : M → R of f to the set
% M := {(x1, x2) ∈ R2 : g(x1, x2) = 0}.
% Exercise 20 (4 Points)
% Let f : R3 → R be a function defined by
% f(x1, x2, x3) := x1 − 2x2 + x2
% 3 ,
% and consider the set M := {x = (x1, x2, x3) ∈ R3 : 󰀂x󰀂2 = 1}. Let f|M : M → R be the restriction
% of f to M.
% (i) Why does f|M : M → R have at least one minimum point and at least one maximum point?
% (ii) Use the method of Lagrange multipliers to determine the extremum points of f|M : M → R.

\section*{Exercise 17}
We are given the function
\begin{align*}
   f(x) := -e^x + 2x + 1.
\end{align*}

We calculte the newtons method using this formula
\begin{align*}
   x_{n} = x_{n-1} - \frac{f(x_{n-1})}{f'(x_{n-1})}
\end{align*}

\subsection*{(i)}
We want to find \(x_2\) when \(x_0 = -1\) \\

Lets start by finding \(f' (x)\)
\begin{align*}
   f'(x) = -e^x + 2
\end{align*}

Now we can calculate \(x_1\) and \(x_2\)
\begin{align*}
   x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} \\
       &= -1 - \frac{-e^{-1} + 2(-1) + 1}{-e^{-1} + 2} \\
         &= -1 - \frac{-\frac{1}{e} - 2 + 1}{-\frac{1}{e} + 2} \\
         &= -1 - \frac{-\frac{1}{e} - 1}{-\frac{1}{e} + 2} \\
         &= -1 + \frac{\frac{1}{e} + 1}{-\frac{1}{e} + 2} \\
         &= -1 + \frac{e + 1}{-1 + 2e} \\
         &= \frac{-2e + e + 1 + 1}{-1 + 2e} \\
         &= \frac{-e + 2}{-1 + 2e} \\
         &\approx -0.16190048965915385 \quad \text{(via using a calculator)} \\
         \\
   x_2 &= x_1 - \frac{f(x_1)}{f'(x_1)} \\
      &\approx -0.16190048965915385 - \frac{f(-0.16190048965915385)}{f'(-0.16190048965915385)} \\
      &\approx -0.16190048965915385 - \frac{-0.174326815763479123074}{f'(-0.16190048965915385)} \quad \text{again using a calculator} \\
      &\approx -0.16190048965915385 - \frac{-0.174326815763479123074}{1.149474163554828576926} \quad \text{again using a calculator} \\
      &\approx -0.16190048965915385 - -0.1516578808734259448068901476583655229970567382438862903207901761438327331 \quad \text{again using a calculator} \\
      &\approx -0.0102426087857279051931098523416344770029432617561137096792098238561672669 \quad \text{again using a calculator} \\
\end{align*}
\\
Links to the calculator: \\
   \url{https://www.wolframalpha.com/input?i=f%28x%29+%3D+-e%5Ex+%2B2x+%2B1+at+x%3D%E2%88%920.16190048965915385&assumption=%7B%22C%22%2C+%22at%22%7D+-%3E+%7B%22EnglishWord%22%7D} \\
   \url{https://www.wolframalpha.com/input?i=g%28x%29+%3D+-e%5Ex+%2B2+where+x+%3D+%E2%88%920.16190048965915385}

\subsection*{(ii)}
We want to find \(x_2\) when \(x_0 = 1\) \\
The derivatie
\begin{align*}
   f'(x) = -e^x + 2
\end{align*}

Now we can calculate \(x_1\) and \(x_2\)
\begin{align*}
   x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} \\
       &= 1 - \frac{-e^{1} + 2(1) + 1}{-e^{1} + 2} \\
         &= 1 - \frac{-e + 2 + 1}{-e + 2} \\
         &= 1 - \frac{-e + 3}{-e + 2} \\
         &= 1 + \frac{e - 3}{-e + 2} \\
         &= \frac{-e + 2 + e - 3}{-e + 2} \\
         &= \frac{-1}{-e + 2} \\
         &\approx 1.3922111911 \quad \text{(via using a calculator)} \\
         \\
   x_2 &= x_1 - \frac{f(x_1)}{f'(x_1)} \\
      &= 1.3922111911 - \frac{f(1.3922111911)}{f'(1.3922111911)} \\
      &\approx 1.3922111911 - \frac{-0.23931509377336}{f'(1.3922111911)} \quad \text{again using a calculator} \\
      &\approx 1.3922111911 - \frac{-0.23931509377336}{-2.02373747597336} \quad \text{again using a calculator} \\
      &\approx 1.3922111911 - 0.1182540208967846811355339766932549 \quad \text{again using a calculator} \\
      &\approx 1.2739571702032153188644660233067451 \quad \text{again using a calculator} \\
\end{align*}

\section*{Exercise 18}
We are given the function
\begin{align*}
   f(x_1, x_2) :=
   \begin{pmatrix}
      x_1^3 + x_2 - 2 \\
      x_1 + x_2^3 - 2
   \end{pmatrix}
\end{align*}

The newtons method for multivariable functions is given by
\begin{align*}
   \mathbf{x_n} = \mathbf{x_{n-1}} - J_f(\mathbf{x_{n-1}})^{-1} f(\mathbf{x_{n-1}})
\end{align*}

The Jacobian matrix \(J_f\) is
\begin{align*}
   J_f(x_1, x_2) &=
   \begin{pmatrix}
      \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
      \\
      \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2}
   \end{pmatrix} \\
   &= \begin{pmatrix}
      3x_1^2 & 1 \\
      1 & 3x_2^2
   \end{pmatrix}
\end{align*}

We start with \(\mathbf{x_0} = (1, 1)\)
\begin{align*}
   \mathbf{x_1} &= \mathbf{x_0} - J_f(\mathbf{x_0})^{-1} f(\mathbf{x_0}) \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix} - \begin{pmatrix}
      3(1)^2 & 1 \\
      1 & 3(1)^2
   \end{pmatrix}^{-1} \begin{pmatrix}
      (1)^3 + (1) - 2 \\
      (1) + (1)^3 - 2
   \end{pmatrix} \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix} - \begin{pmatrix}
      3 & 1 \\
      1 & 3
   \end{pmatrix}^{-1} \begin{pmatrix}
      0 \\ 0
   \end{pmatrix} \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix} - \begin{pmatrix}
      0 \\ 0
   \end{pmatrix} \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix}
\end{align*}

Doing the same calculation \(\mathbf{x_2}\) will also be \(\begin{pmatrix}1 \\ 1\end{pmatrix}\). It turns out that i didnt even need to calculate the jacobian
matrix :)

\section*{Exercise 19}
We are given the functions
\begin{align*}
   f(x_1, x_2) &:= \frac{1}{3} x_1^3 + \frac{1}{4} x_2^2 + x_1 + \frac{1}{2} x_2 + 2 , \\
   g(x_1, x_2) &:= x_1 + \frac{1}{2} x_2.
\end{align*}

Choose \(h(x_1) = -2x_1\) so that \(g(x_1, h(x_1)) = 0\).
\begin{align*}
   f |_M (x_1) &= f(x_1, h(x_1)) \\
   &= \frac{1}{3} x_1^3 + \frac{1}{4} (-2x_1)^2 + x_1 + \frac{1}{2} (-2x_1) + 2 \\
   &= \frac{1}{3} x_1^3 + \frac{1}{4} (4x_1^2) + x_1 - x_1 + 2 \\
   &= \frac{1}{3} x_1^3 + x_1^2 + 2 \\
\end{align*}

Now we need to find the extremum points of \(f |_M\). Lets continue with derivaties
\begin{align*}
   f |_M' (x_1) &= x_1^2 + 2x_1 \\
   f |_M'' (x_1) &= 2x_1 + 2
\end{align*}

Setting the first derivatie to zero
\begin{align*}
   f |_M' (x_1) = 0 &\implies x_1^2 + 2x_1 = 0 \\
   &\implies x_1 (x_1 + 2) = 0 \\
   &\implies x_1 = 0 \quad \text{or} \quad x_1 = -2
\end{align*}

Now we can use the second derivatie test to classify these points
\begin{itemize}
   \item For \(x_1 = 0\):
   \begin{align*}
      f |_M'' (0) = 2(0) + 2 = 2 > 0
   \end{align*}
   So we have a local minimum at \(x_1 = 0\). The corresponding \(x_2\) value is
   \begin{align*}
      x_2 = h(0) = -2(0) = 0
   \end{align*}
   Thus one local minimum point is \((0, 0)\).
   \item For \(x_1 = -2\):
   \begin{align*}
      f |_M'' (-2) = 2(-2) + 2 = -2 < 0
   \end{align*}
   So we have a local maximum at \(x_1 = -2\). The corresponding \(x_2\) value is
   \begin{align*}
      x_2 = h(-2) = -2(-2) = 4
   \end{align*}
   Thus one local maximum point is \((-2, 4)\).
\end{itemize}

\section*{Exercise 20}
We are given the function
\begin{align*}
   f(x_1, x_2, x_3) := x_1 - 2x_2 + x_3^2
\end{align*}

and the set
\begin{align*}
   M := \{\mathbf{x} = (x_1, x_2, x_3) \in \mathbb{R}^3 : \|\mathbf{x}\|^2 = 1\}
\end{align*}

\subsection*{(i)}
The set \(M\) is closed and bounded in \(\mathbb{R}^3\). So by the extreme value theorem the continuous function \(f|_M : M \to \mathbb{R}\)
also has a maximum and a minimum on \(M\). \\
\\
Why \(f\) is continous is obvious because its a polynomial function. \\
\\
Why \(M\) is closed: intuitively the set \(M\) is just points on the surface of the unit sphere. And if you think about a random point in the complement space
then we can find a ball around that point that doesnt touch the surface of the sphere. And since complement of \(M\) is open that makes the \(M\) closed. \\
\\
Why \(M\) is bounded: again intuitively the set \(M\) is just points on the surface of the unit sphere and thats obviously bounded.

\subsection*{(ii)}
Let \(g(x_1, x_2, x_3) = \|\mathbf{x}\| - 1\). so that we have a name for the constraint of \(M\). But notice here that i dropped the square
because norm is always positive so its ok to do that.

The lagrange function is
\begin{align*}
   L: \mathbb{R}^4 \rightarrow \mathbb{R}, \qquad L(x_1, x_2, x_3, \lambda) = f(x_1, x_2, x_3) + \lambda g(x_1, x_2, x_3)
\end{align*}

\begin{align*}
   L(x_1, x_2, x_3, \lambda) &= x_1 - 2x_2 + x_3^2 + \lambda (\sqrt{x_1^2 + x_2^2 + x_3^2} - 1) \\
\end{align*}

The gradient of \(L\)
\begin{align*}
   \nabla L(x_1, x_2, x_3, \lambda) =
   \begin{pmatrix}
      1 + \lambda \frac{x_1}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \\
      -2 + \lambda \frac{x_2}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \\
      2x_3 + \lambda \frac{x_3}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \\
      \sqrt{x_1^2 + x_2^2 + x_3^2} - 1
   \end{pmatrix}
\end{align*}

Setting the gradient to zero
\begin{align*}
   \nabla L(x_1, x_2, x_3, \lambda) = 0 &\implies
   \begin{pmatrix}
      1 + \lambda \frac{x_1}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \\
      -2 + \lambda \frac{x_2}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \\
      2x_3 + \lambda \frac{x_3}{\sqrt{x_1^2 + x_2^2 + x_3^2}} \\
      \sqrt{x_1^2 + x_2^2 + x_3^2} - 1
   \end{pmatrix} = 0 \\
   &\implies
   \begin{cases}
      1 + \lambda \frac{x_1}{\sqrt{x_1^2 + x_2^2 + x_3^2}} = 0 \\
      -2 + \lambda \frac{x_2}{\sqrt{x_1^2 + x_2^2 + x_3^2}} = 0 \\
      2x_3 + \lambda \frac{x_3}{\sqrt{x_1^2 + x_2^2 + x_3^2}} = 0 \\
      \sqrt{x_1^2 + x_2^2 + x_3^2} - 1 = 0
   \end{cases} \\
   \intertext{Subsitute the last equationinto first three}
   &\implies
   \begin{cases}
      1 + \lambda x_1 = 0 \\
      -2 + \lambda x_2 = 0 \\
      2x_3 + \lambda x_3 = 0 \\
      x_1^2 + x_2^2 + x_3^2 = 1
   \end{cases} \\
   &\implies
   \begin{cases}
      x_1 = -\frac{1}{\lambda} \\
      x_2 = \frac{2}{\lambda} \\
      x_3 (2 + \lambda) = 0 \\
      x_1^2 + x_2^2 + x_3^2 = 1
   \end{cases}
\end{align*}

From the third equation we have two cases:
\begin{itemize}
   \item If \(x_3 = 0\):
   \begin{align*}
      x_1^2 + x_2^2 + x_3^2 = 1 &\implies \left(-\frac{1}{\lambda}\right)^2 + \left(\frac{2}{\lambda}\right)^2 + 0^2 = 1 \\
      &\implies \frac{1}{\lambda^2} + \frac{4}{\lambda^2} = 1 \\
      &\implies \frac{5}{\lambda^2} = 1 \\
      &\implies \lambda^2 = 5 \\
      &\implies \lambda = \sqrt{5} \quad \text{or} \quad \lambda = -\sqrt{5}
   \end{align*}
   For \(\lambda = \sqrt{5}\):
   \begin{align*}
      x_1 = -\frac{1}{\sqrt{5}}, \quad x_2 = \frac{2}{\sqrt{5}}, \quad x_3 = 0
   \end{align*}
   For \(\lambda = -\sqrt{5}\):
   \begin{align*}
      x_1 = \frac{1}{\sqrt{5}}, \quad x_2 = -\frac{2}{\sqrt{5}}, \quad x_3 = 0
   \end{align*}
   \item If \(\lambda = -2\):
   \begin{align*}
      x_1 = -\frac{1}{-2} = \frac{1}{2}, \quad x_2 = \frac{2}{-2} = -1, \quad x_3 \text{ is free}
   \end{align*}
   Using the constraint equation:
   \begin{align*}
      x_1^2 + x_2^2 + x_3^2 = 1 &\implies \left(\frac{1}{2}\right)^2 + (-1)^2 + x_3^2 = 1 \\
      &\implies \frac{1}{4} + 1 + x_3^2 = 1 \\
      &\implies x_3^2 = 1 - \frac{5}{4} = -\frac{1}{4}
   \end{align*}
   which has no real solution.
\end{itemize}

Long story short the extremum points are
\begin{align*}
   \left(-\frac{1}{\sqrt{5}}, \frac{2}{\sqrt{5}}, 0\right) \quad \text{and} \quad \left(\frac{1}{\sqrt{5}}, -\frac{2}{\sqrt{5}}, 0\right)
\end{align*}

\end{document}