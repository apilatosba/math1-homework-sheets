\documentclass{article}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{tikz}
\usepackage{array}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{hyperref}

\begin{document}
\section*{\huge Homework Sheet 5}
\begin{flushright}
   \textbf{Author: Abdullah Oğuz Topçuoğlu}
\end{flushright}

% Exercise 17 (4 Points)
% Let f : R → R be a function defined by
% f(x) := −ex + 2x + 1.
% Solve the equation f(x) = 0 approximately by using two iterations of the Newton method with the
% initial value
% (i) x0 := −1.
% (ii) x0 := 1.
% Exercise 18 (4 Points)
% Let f : R2 → R2 be a function defined by
% f(x1, x2) :=
% 󰀗 x3
% 1 + x2 − 2
% x1 + x3
% 2 − 2
% 󰀘
% .
% Solve the equation f(x1, x2) = (0, 0) approximately by using two iterations of the Newton method
% with the initial value x0 := (1, 1).
% Exercise 19 (4 Points)
% Let the functions f : R2 → R and g : R2 → R be defined by
% f(x1, x2) := 1
% 3
% x3
% 1 +
% 1
% 4
% x2
% 2 + x1 +
% 1
% 2
% x2 + 2 ,
% g(x1, x2) := x1 +
% 1
% 2
% x2.
% Use the substitution method to determine the local extremum points of f under the constraint
% g(x1, x2) = 0, i.e. to find all local extremum points of the restriction f|M : M → R of f to the set
% M := {(x1, x2) ∈ R2 : g(x1, x2) = 0}.
% Exercise 20 (4 Points)
% Let f : R3 → R be a function defined by
% f(x1, x2, x3) := x1 − 2x2 + x2
% 3 ,
% and consider the set M := {x = (x1, x2, x3) ∈ R3 : 󰀂x󰀂2 = 1}. Let f|M : M → R be the restriction
% of f to M.
% (i) Why does f|M : M → R have at least one minimum point and at least one maximum point?
% (ii) Use the method of Lagrange multipliers to determine the extremum points of f|M : M → R.

\section*{Exercise 17}
We are given the function
\begin{align*}
   f(x) := -e^x + 2x + 1.
\end{align*}

We calculte the newtons method using this formula
\begin{align*}
   x_{n} = x_{n-1} - \frac{f(x_{n-1})}{f'(x_{n-1})}
\end{align*}

\subsection*{(i)}
We want to find \(x_2\) when \(x_0 = -1\) \\

Lets start by finding \(f' (x)\)
\begin{align*}
   f'(x) = -e^x + 2
\end{align*}

Now we can calculate \(x_1\) and \(x_2\)
\begin{align*}
   x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} \\
       &= -1 - \frac{-e^{-1} + 2(-1) + 1}{-e^{-1} + 2} \\
         &= -1 - \frac{-\frac{1}{e} - 2 + 1}{-\frac{1}{e} + 2} \\
         &= -1 - \frac{-\frac{1}{e} - 1}{-\frac{1}{e} + 2} \\
         &= -1 + \frac{\frac{1}{e} + 1}{-\frac{1}{e} + 2} \\
         &= -1 + \frac{e + 1}{-1 + 2e} \\
         &= \frac{-2e + e + 1 + 1}{-1 + 2e} \\
         &= \frac{-e + 2}{-1 + 2e} \\
         &\approx -0.16190048965915385 \quad \text{(via using a calculator)} \\
         \\
   x_2 &= x_1 - \frac{f(x_1)}{f'(x_1)} \\
      &\approx -0.16190048965915385 - \frac{f(-0.16190048965915385)}{f'(-0.16190048965915385)} \\
      &\approx -0.16190048965915385 - \frac{-0.174326815763479123074}{f'(-0.16190048965915385)} \quad \text{again using a calculator} \\
      &\approx -0.16190048965915385 - \frac{-0.174326815763479123074}{1.149474163554828576926} \quad \text{again using a calculator} \\
      &\approx -0.16190048965915385 - -0.1516578808734259448068901476583655229970567382438862903207901761438327331 \quad \text{again using a calculator} \\
      &\approx -0.0102426087857279051931098523416344770029432617561137096792098238561672669 \quad \text{again using a calculator} \\
\end{align*}
\\
Links to the calculator: \\
   \url{https://www.wolframalpha.com/input?i=f%28x%29+%3D+-e%5Ex+%2B2x+%2B1+at+x%3D%E2%88%920.16190048965915385&assumption=%7B%22C%22%2C+%22at%22%7D+-%3E+%7B%22EnglishWord%22%7D} \\
   \url{https://www.wolframalpha.com/input?i=g%28x%29+%3D+-e%5Ex+%2B2+where+x+%3D+%E2%88%920.16190048965915385}

\subsection*{(ii)}
We want to find \(x_2\) when \(x_0 = 1\) \\
The derivatie
\begin{align*}
   f'(x) = -e^x + 2
\end{align*}

Now we can calculate \(x_1\) and \(x_2\)
\begin{align*}
   x_1 &= x_0 - \frac{f(x_0)}{f'(x_0)} \\
       &= 1 - \frac{-e^{1} + 2(1) + 1}{-e^{1} + 2} \\
         &= 1 - \frac{-e + 2 + 1}{-e + 2} \\
         &= 1 - \frac{-e + 3}{-e + 2} \\
         &= 1 + \frac{e - 3}{-e + 2} \\
         &= \frac{-e + 2 + e - 3}{-e + 2} \\
         &= \frac{-1}{-e + 2} \\
         &\approx 1.3922111911 \quad \text{(via using a calculator)} \\
         \\
   x_2 &= x_1 - \frac{f(x_1)}{f'(x_1)} \\
      &= 1.3922111911 - \frac{f(1.3922111911)}{f'(1.3922111911)} \\
      &\approx 1.3922111911 - \frac{-0.23931509377336}{f'(1.3922111911)} \quad \text{again using a calculator} \\
      &\approx 1.3922111911 - \frac{-0.23931509377336}{-2.02373747597336} \quad \text{again using a calculator} \\
      &\approx 1.3922111911 - 0.1182540208967846811355339766932549 \quad \text{again using a calculator} \\
      &\approx 1.2739571702032153188644660233067451 \quad \text{again using a calculator} \\
\end{align*}

\section*{Exercise 18}
We are given the function
\begin{align*}
   f(x_1, x_2) :=
   \begin{pmatrix}
      x_1^3 + x_2 - 2 \\
      x_1 + x_2^3 - 2
   \end{pmatrix}
\end{align*}

The newtons method for multivariable functions is given by
\begin{align*}
   \mathbf{x_n} = \mathbf{x_{n-1}} - J_f(\mathbf{x_{n-1}})^{-1} f(\mathbf{x_{n-1}})
\end{align*}

The Jacobian matrix \(J_f\) is
\begin{align*}
   J_f(x_1, x_2) &=
   \begin{pmatrix}
      \frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} \\
      \\
      \frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2}
   \end{pmatrix} \\
   &= \begin{pmatrix}
      3x_1^2 & 1 \\
      1 & 3x_2^2
   \end{pmatrix}
\end{align*}

We start with \(\mathbf{x_0} = (1, 1)\)
\begin{align*}
   \mathbf{x_1} &= \mathbf{x_0} - J_f(\mathbf{x_0})^{-1} f(\mathbf{x_0}) \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix} - \begin{pmatrix}
      3(1)^2 & 1 \\
      1 & 3(1)^2
   \end{pmatrix}^{-1} \begin{pmatrix}
      (1)^3 + (1) - 2 \\
      (1) + (1)^3 - 2
   \end{pmatrix} \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix} - \begin{pmatrix}
      3 & 1 \\
      1 & 3
   \end{pmatrix}^{-1} \begin{pmatrix}
      0 \\ 0
   \end{pmatrix} \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix} - \begin{pmatrix}
      0 \\ 0
   \end{pmatrix} \\
   &= \begin{pmatrix}1 \\ 1\end{pmatrix}
\end{align*}

Doing the same calculation \(\mathbf{x_2}\) will also be \(\begin{pmatrix}1 \\ 1\end{pmatrix}\). It turns out that i didnt even need to calculate the jacobian
matrix :)

\section*{Exercise 19}
We are given the functions
\begin{align*}
   f(x_1, x_2) &:= \frac{1}{3} x_1^3 + \frac{1}{4} x_2^2 + x_1 + \frac{1}{2} x_2 + 2 , \\
   g(x_1, x_2) &:= x_1 + \frac{1}{2} x_2.
\end{align*}

Choose \(h(x_1) = -2x_1\) so that \(g(x_1, h(x_1)) = 0\).
\begin{align*}
   f |_M (x_1) &= f(x_1, h(x_1)) \\
   &= \frac{1}{3} x_1^3 + \frac{1}{4} (-2x_1)^2 + x_1 + \frac{1}{2} (-2x_1) + 2 \\
   &= \frac{1}{3} x_1^3 + \frac{1}{4} (4x_1^2) + x_1 - x_1 + 2 \\
   &= \frac{1}{3} x_1^3 + x_1^2 + 2 \\
\end{align*}

Now we need to find the extremum points of \(f |_M\). Lets continue with derivaties
\begin{align*}
   f |_M' (x_1) &= x_1^2 + 2x_1 \\
   f |_M'' (x_1) &= 2x_1 + 2
\end{align*}

Setting the first derivatie to zero
\begin{align*}
   f |_M' (x_1) = 0 &\implies x_1^2 + 2x_1 = 0 \\
   &\implies x_1 (x_1 + 2) = 0 \\
   &\implies x_1 = 0 \quad \text{or} \quad x_1 = -2
\end{align*}

Now we can use the second derivatie test to classify these points
\begin{itemize}
   \item For \(x_1 = 0\):
   \begin{align*}
      f |_M'' (0) = 2(0) + 2 = 2 > 0
   \end{align*}
   So we have a local minimum at \(x_1 = 0\). The corresponding \(x_2\) value is
   \begin{align*}
      x_2 = h(0) = -2(0) = 0
   \end{align*}
   Thus one local minimum point is \((0, 0)\).
   \item For \(x_1 = -2\):
   \begin{align*}
      f |_M'' (-2) = 2(-2) + 2 = -2 < 0
   \end{align*}
   So we have a local maximum at \(x_1 = -2\). The corresponding \(x_2\) value is
   \begin{align*}
      x_2 = h(-2) = -2(-2) = 4
   \end{align*}
   Thus one local maximum point is \((-2, 4)\).
\end{itemize}

\end{document}